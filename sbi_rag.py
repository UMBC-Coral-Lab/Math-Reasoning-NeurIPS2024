# -*- coding: utf-8 -*-
"""SBI_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PTyrahEqwiTGXbVvmbHd-HUXIoLAsyjf
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install colab-xterm
# %load_ext colabxterm

# type these codes in below terminal after run the cell (%xterm)
# curl -fsSL https://ollama.com/install.sh | sh
# ollama serve & ollama pull llama3.1 & ollama pull nomic-embed-text
# pkill ollama
# ps aux | grep ollama

"""<h2>Running and Loading llama 3.1

"""

# Commented out IPython magic to ensure Python compatibility.
# %xterm

!pip -qq install langchain
!pip -qq install langchain-core
!pip -qq install langchain-community

from langchain_community.llms import Ollama
llm = Ollama(model = "llama3.1")
llm.invoke("what is the Meaning of life")

!pip install ollama langchain beautifulsoup4 chromadb gradio -q

!pip install transformers datasets scikit-learn pandas torch gradio langchain_community

!pip install datasets

"""<h1>Schema Classifier trained on a custom Schema Based Dataset</h1>"""

!pip install torch --upgrade

import pandas as pd
from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import gradio as gr
import ollama
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from sklearn.metrics.pairwise import cosine_similarity
import torch

# Load the dataset
df = pd.read_csv("/content/SBI-Dataset.csv")

# Combine schema and sub-category into a single label
df['Combined_Label'] = df['Schema'] + "_" + df['Sub-Category']

# Split the dataset into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples["Problem"], padding="max_length", truncation=True)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Encode the combined labels
label_encoder = LabelEncoder()
train_labels = label_encoder.fit_transform(train_df["Combined_Label"])
test_labels = label_encoder.transform(test_df["Combined_Label"])

train_dataset = train_dataset.add_column("labels", train_labels)
test_dataset = test_dataset.add_column("labels", test_labels)


# Set the format for PyTorch
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# Load the DistilBERT model for sequence classification
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=len(label_encoder.classes_))

# # Define the training arguments with proper logging enabled #Uncomment it when required for training
# training_args = TrainingArguments(
#     output_dir="./results",
#     evaluation_strategy="epoch",
#     logging_strategy="steps",  # Log after every few steps
#     logging_steps=10,  # Log every 10 steps (you can adjust this)
#     learning_rate=2e-5,
#     per_device_train_batch_size=16,
#     per_device_eval_batch_size=16,
#     num_train_epochs=20,
#     weight_decay=0.01,
#     save_strategy="epoch",  # Save model at the end of each epoch
#     report_to="none",  # Disable default logging to avoid conflicts
# )

# # Create a Trainer instance
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     eval_dataset=test_dataset,
# )

# # Train the model
# trainer.train()

# # Evaluate the model
# eval_result = trainer.evaluate()
# print(f"Evaluation results: {eval_result}")

# # Save the model and tokenizer
# model.save_pretrained("./schema_classifier_2")
# tokenizer.save_pretrained("./schema_classifier_2")

# # Load the model and tokenizer for later use
# model = AutoModelForSequenceClassification.from_pretrained("./schema_classifier_2")
# tokenizer = AutoTokenizer.from_pretrained("./schema_classifier_2")

!unzip /content/schema_classifier_20Epochs.zip

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import cross_val_score, KFold
import torch
import matplotlib.pyplot as plt
import seaborn as sns

# Load the model and tokenizer for later use
model = AutoModelForSequenceClassification.from_pretrained("/content/content/schema_classifier_2")
tokenizer = AutoTokenizer.from_pretrained("/content/content/schema_classifier_2")


# Step 2: Tokenize the test dataset (assuming test_dataset is already loaded from your code)
# Convert the test dataset to torch tensors
test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# Create the test dataloader
from torch.utils.data import DataLoader
test_dataloader = DataLoader(test_dataset, batch_size=16)

# Step 3: Evaluate the model and predict the labels
model.eval()  # Set the model to evaluation mode

all_labels = []
all_preds = []

with torch.no_grad():
    for batch in test_dataloader:
        inputs = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}
        labels = batch['labels']

        outputs = model(**inputs)
        logits = outputs.logits

        preds = torch.argmax(logits, dim=-1)

        all_labels.extend(labels.cpu().numpy())
        all_preds.extend(preds.cpu().numpy())

# Step 4: Generate the confusion matrix and classification report
conf_matrix = confusion_matrix(all_labels, all_preds)
class_report = classification_report(all_labels, all_preds, target_names=label_encoder.classes_)

print("Confusion Matrix:")
print(conf_matrix)

print("\nClassification Report:")
print(class_report)

# Step 5: Visualize the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Step 6: Perform cross-validation on the labeled dataset using k-fold
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Function to get validation accuracy in each fold
def get_model_predictions(model, dataset):
    all_preds = []
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():
        for batch in dataset:
            inputs = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}
            outputs = model(**inputs)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=-1)
            all_preds.extend(preds.cpu().numpy())
    return all_preds

# Applying k-fold cross-validation
accuracies = []
for train_idx, test_idx in kf.split(df):
    # Split the dataset into train and test sets for the current fold
    train_data = df.iloc[train_idx]
    test_data = df.iloc[test_idx]

    # Tokenize the test data for this fold
    test_dataset_fold = Dataset.from_pandas(test_data)
    test_dataset_fold = test_dataset_fold.map(tokenize_function, batched=True)
    test_labels_fold = label_encoder.transform(test_data["Combined_Label"])
    test_dataset_fold = test_dataset_fold.add_column("labels", test_labels_fold)
    test_dataset_fold.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

    # Create dataloader for this fold
    test_dataloader_fold = DataLoader(test_dataset_fold, batch_size=16)

    # Get predictions for this fold
    preds_fold = get_model_predictions(model, test_dataloader_fold)

    # Calculate accuracy
    accuracy_fold = np.mean(preds_fold == test_labels_fold)
    accuracies.append(accuracy_fold)

# Step 7: Report the cross-validation results
print(f"Cross-validation accuracies for each fold: {accuracies}")
print(f"Mean cross-validation accuracy: {np.mean(accuracies)}")

# Visualizing cross-validation accuracies
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(accuracies) + 1), accuracies, marker='o', linestyle='--', color='b')
plt.title('Cross-Validation Accuracy per Fold')
plt.xlabel('Fold Number')
plt.ylabel('Accuracy')
plt.xticks(range(1, len(accuracies) + 1))
plt.show()

import pandas as pd
from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import gradio as gr
import ollama
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from sklearn.metrics.pairwise import cosine_similarity
import torch
# Load the model and tokenizer for later use
model = AutoModelForSequenceClassification.from_pretrained("/content/content/schema_classifier_2")
tokenizer = AutoTokenizer.from_pretrained("/content/content/schema_classifier_2")

# Load the data from the web URL
url = 'https://iris.peabody.vanderbilt.edu/module/math/cresource/q2/p06/' #Document for RAG Context Retrival
loader = WebBaseLoader(url)
docs = loader.load()

# Split the loaded documents into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)

# Create Ollama embeddings and vector store
embeddings = OllamaEmbeddings(model="nomic-embed-text")
vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)

# Function to predict the schema and sub-category using the trained model
def predict_schema_and_subcategory(problem):
    inputs = tokenizer(problem, return_tensors="pt", padding="max_length", truncation=True)
    outputs = model(**inputs)
    predicted_label = outputs.logits.argmax(axis=-1).item()
    combined_label = label_encoder.inverse_transform([predicted_label])[0]
    schema, sub_category = combined_label.split("_")
    return schema, sub_category

# Function to retrieve context with consideration of schema and sub-category
def retrieve_context_with_schema(question):
    schema, sub_category = predict_schema_and_subcategory(question)
    prompt = f"Using the {schema} schema and {sub_category} sub-category, solve the following problem: {question}"
    question_embedding = embeddings.embed_documents([prompt])[0]
    print("Question Embedding:", question_embedding)
    retrieved_docs = vectorstore.similarity_search(prompt)
    ranked_docs = advanced_rerank(retrieved_docs, question_embedding)
    print("Ranked Docs:", ranked_docs)
    context = "\n\n".join(doc.page_content for doc in ranked_docs[:3])
    # print("Context used for LLM:\n", context)
    print("Total embeddings retrieved:", len(retrieved_docs))  # Print the total number of embeddings retrieved
    return schema, sub_category, context

# # Define the function to call the Ollama Llama3.1 model
# def ollama_llm(question, context):
#     # formatted_prompt = f"Based on the following context, solve the problem:\n\n{context}\n\nProblem: {question}"
#     formatted_prompt = f"Using the {schema} schema and {sub_category} sub-category, solve the following problem:\n\nProblem: {question}"

#     response = ollama.chat(model='llama3.1', messages=[{'role': 'user', 'content': formatted_prompt}])
#     return response['message']['content']

# # Redefine the RAG chain to use schema-based context retrieval
# def rag_chain(question):
#     try:
#         schema, sub_category, context = retrieve_context_with_schema(question)
#         response = ollama_llm(question, context)
#         return schema, sub_category, response
#     except Exception as e:
#         return 'Error', 'Error', f"Error in RAG chain processing: {e}"

def ollama_llm(question, schema, sub_category, context):
    # Use a structured prompt that includes schema and sub-category
    formatted_prompt = (
        f"Using the {schema} schema and {sub_category} sub-category, "
        f"solve the following problem:\n\nProblem: {question}\n\n"
        f"Context:\n{context}\n\n"
        "Please fill in the [Solution Steps] and [Answer]."
    )

    response = ollama.chat(model='llama3.1', messages=[{'role': 'user', 'content': formatted_prompt}])
    return response['message']['content']

# Redefine the RAG chain to use schema-based context retrieval
def rag_chain(question):
    try:
        schema, sub_category, context = retrieve_context_with_schema(question)
        response = ollama_llm(question, schema, sub_category, context)
        return schema, sub_category, response
    except Exception as e:
        return 'Error', 'Error', f"Error in RAG chain processing: {e}"

# Advanced re-ranking based on cosine similarity
def advanced_rerank(retrieved_docs, question_embedding):
    try:
        doc_embeddings = [embeddings.embed_documents([doc.page_content])[0] for doc in retrieved_docs]
        similarities = [cosine_similarity([question_embedding], [doc_embedding])[0][0] for doc_embedding in doc_embeddings]
        # print("Similarities:", similarities)  # Print the similarities for debugging
        # Re-rank documents based on similarity scores
        ranked_docs = sorted(zip(similarities, retrieved_docs), key=lambda x: x[0], reverse=True)
        return [doc for _, doc in ranked_docs]
    except Exception as e:
        print(f"Error during document re-ranking: {e}")
        return retrieved_docs

# Gradio interface
def get_important_facts(question):
    try:
        schema, sub_category, response = rag_chain(question)
        return f"Schema used: {schema}\nSub-category used: {sub_category}\nAnswer: {response}"
    except Exception as e:
        return f"An error occurred: {e}"

iface = gr.Interface(
    fn=get_important_facts,
    inputs=gr.Textbox(lines=2, placeholder="Enter your question here..."),
    outputs="text",
    title="RAG with Llama3.1",
    description="Ask questions about the provided context",
)

iface.launch()

def calculate_reasoning_score(generated_answer):
    # Custom logic to check if all steps are present, valid, and clear
    steps = ["+", "*", "Total","Schema used:","Sub-category used:","Additive","Multiplicative","Steps","Total","known","unknowns","ratio","Ratios/Proportions","multiplication","ratio","-","/"]
    matching_steps = sum(1 for step in steps if step in generated_answer)
    clarity_factor = 0.1 * len(generated_answer.split())  # Example clarity factor
    reasoning_score = (matching_steps / len(steps)) * (1 if clarity_factor > 0.5 else 0.8)
    return reasoning_score

"""<h1>Reasoning Score and delta scoring</h1>"""

def calculate_delta_score(generated_answer):

    # # Define expected transitions between steps
    # transitions = [
    #     ("jaguars", "snakes"),
    #     ("snakes", "birds"),
    #     ("birds", "beetles")
    # ]

#     transitions = [
#     ("Jacob", "24 years"),
#     ("Tony", "half of Jacob"),
#     ("years later", "6"),
#     ("Tony's age", "calculated")
# ]

    transitions = [
        ("James", "40 years"),
        ("partner", "10 years less"),
        ("partner's experience", "30 years"),
        ("combined experience", "70 years")
    ] # transition for the question : "James spends 40 years teaching. His partner has been teaching for 10 years less. How long is their combined experience? "

    #  transitions = [
    #     "Transition for your questions"
    # ]


    # Calculate delta score by checking transitions between steps
    delta_score = 0
    for start, end in transitions:
        if start in generated_answer.lower() and end in generated_answer.lower():
            delta_score += 1

    delta_score = delta_score / len(transitions)  # Normalize by number of transitions
    return delta_score

def calculate_reasoning_score(generated_answer):
    # Custom logic to check if all steps are present, valid, and clear
    steps = ["+", "*", "Total", "Schema used:", "Sub-category used:", "Additive",
             "Multiplicative", "Steps", "Total", "known", "unknowns", "ratio",
             "Ratios/Proportions", "multiplication", "ratio", "-", "/","Equal Groups"]

    # Count how many expected steps are present in the generated answer
    matching_steps = sum(1 for step in steps if step in generated_answer)

    # Calculate clarity factor based on explanation length
    clarity_factor = 0.1 * len(generated_answer.split())  # Example clarity factor

    # Calculate delta score
    delta_score = calculate_delta_score(generated_answer)
    print("Delta Score:", delta_score)

    # Calculate final reasoning score with delta consideration
    reasoning_score = (matching_steps / len(steps)) * (1 if clarity_factor > 0.5 else 0.8) * delta_score
    return reasoning_score

"""<h1>SBI-RAG Response : Paste your Response from the above  </h1>
<h1>LLM Model Response : Paste the Response from the GPT Model </h1>
"""

"# Example responses to evaluate
schema_based_rag_response = """
Schema used: Additive
Sub-category used: Total
Answer: Here's a solution to the problem using the Additive schema and Total sub-category:

**Problem:** James spends 40 years teaching. His partner has been teaching for 10 years less. How long is their combined experience?

**Step 1: Identify the total**

* James' teaching experience: 40 years
* Partner's teaching experience (less than James'): 40 - 10 = 30 years

**Step 2: Combine the totals**

* Combined experience = James' experience + partner's experience
* Combined experience = 40 + 30
* Combined experience = 70 years

Therefore, their combined experience is **70 years**.
"""

llm_response = """
James has been teaching for 40 years, and his partner has been teaching for
40
−
10
=
30
40−10=30 years.

Their combined teaching experience is:

40
+
30
=
70
 years
40+30=70 years
So, together, they have 70 years of teaching experience.

"""

# Calculate reasoning scores for both responses
schema_based_rag_score = calculate_reasoning_score(schema_based_rag_response)


schema_based_rag_score

llm_score = calculate_reasoning_score(llm_response)
llm_score

import matplotlib.pyplot as plt
import numpy as np

# Metrics for RAG and LLM responses
metrics = ['Reasoning Score']
rag_values = [calculate_reasoning_score(schema_based_rag_response)]
llm_values = [0.4908123]  # Example values

# Bar chart
x = np.arange(len(metrics))  # Label locations
width = 0.35  # Width of bars

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, rag_values, width, label='SBI-RAG')
rects2 = ax.bar(x + width/2, llm_values, width, label='LLM Response(GPT 4)')

# Add labels, title, and custom x-axis tick labels
ax.set_ylabel('Scores')
ax.set_title('Comparison of SBI-RAG vs LLM reasoning')
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.legend(prop={'size': 8})

plt.show()

"""<h2> T-Test for Statistical Significance</h2>

"""

import pandas as pd
from scipy import stats

# Load the data from the Excel file
file_path = '/content/Paired - t test one tail.xlsx'
excel_data = pd.ExcelFile(file_path)

# Parse the relevant sheet
df = excel_data.parse('Sheet1')

# Extract the two columns for the paired t-test
sbi_rag_scores = df['SBI-RAG Reasoning Score '].astype(float)
gpt_4_scores = df['GPT 4.0'].astype(float)

# Perform a one-tailed paired t-test (alternative hypothesis: mean SBI-RAG score > mean GPT 4.0 score)
t_statistic, p_value = stats.ttest_rel(sbi_rag_scores, gpt_4_scores, alternative='greater')

# Output the results
print(f"t-statistic: {t_statistic}")
print(f"p-value: {p_value}")

import pandas as pd
from scipy import stats

# Load the data from the Excel file
file_path = '/content/Paired - t test one tail.xlsx'
excel_data = pd.ExcelFile(file_path)

# Parse the relevant sheet
df = excel_data.parse('Sheet1')

# Extract the columns for the paired t-test
sbi_rag_scores = df['SBI-RAG Reasoning Score '].astype(float)
gpt_4_scores = df['GPT 4.0 Reasoning Score'].astype(float)
gpt_3_5_scores = df['GPT 3.5 Turbo Reasoning Score'].astype(float)

# Perform a one-tailed paired t-test for GPT 4.0 (alternative hypothesis: mean SBI-RAG score > mean GPT 4.0 score)
t_statistic_gpt4, p_value_gpt4 = stats.ttest_rel(sbi_rag_scores, gpt_4_scores, alternative='greater')

# Perform a one-tailed paired t-test for GPT 3.5 Turbo (alternative hypothesis: mean SBI-RAG score > mean GPT 3.5 Turbo score)
t_statistic_gpt3_5, p_value_gpt3_5 = stats.ttest_rel(sbi_rag_scores, gpt_3_5_scores, alternative='greater')

t_statistic_gpt4_new, p_value_gpt4_new = stats.ttest_rel(gpt_4_scores, gpt_3_5_scores, alternative='greater')

# Output the results
print("Comparison of SBI-RAG Reasoning Score with GPT 4.0:")
print(f"t-statistic (GPT 4.0): {t_statistic_gpt4}")
print(f"p-value (GPT 4.0): {p_value_gpt4}")

print("\nComparison of SBI-RAG Reasoning Score with GPT 3.5 Turbo:")
print(f"t-statistic (GPT 3.5 Turbo): {t_statistic_gpt3_5}")
print(f"p-value (GPT 3.5 Turbo): {p_value_gpt3_5}")

print("\nComparison of GPT-4 Reasoning Score with GPT 3.5 Turbo:")
print(f"t-statistic (GPT 3.5 Turbo_vs_GPT_4): {t_statistic_gpt4_new}")
print(f"p-value (GPT 3.5 Turbo_vs_GPT_4): {p_value_gpt4_new}")

import matplotlib.pyplot as plt

# Plotting the scores for visual comparison
plt.figure(figsize=(10, 6))

# Plotting SBI-RAG and GPT 4.0 scores as paired lines for each data point
plt.plot(sbi_rag_scores, label='SBI-RAG Reasoning Score', marker='o', linestyle='-', color='blue')
plt.plot(gpt_4_scores, label='GPT 4.0 Score', marker='o', linestyle='-', color='green')

# Adding title and labels
plt.title('Comparison of SBI-RAG Reasoning Scores and GPT 4.0 Scores')
plt.xlabel('Data Points')
plt.ylabel('Scores')
plt.legend()

# Display the plot
plt.show()

import pandas as pd
from scipy import stats

# Load the data from the Excel file
file_path = '/content/Paired - t test one tail.xlsx'
excel_data = pd.ExcelFile(file_path)

# Parse the relevant sheet
df = excel_data.parse('Sheet1')

# Extract the columns for the paired t-test
sbi_rag_scores = df['SBI-RAG Reasoning Score '].astype(float)
gpt_3_5_scores = df['GPT 3.5 Turbo'].astype(float)
gpt_4_scores = df['GPT 4.0'].astype(float)




# Perform one-tailed paired t-tests (alternative hypothesis: mean SBI-RAG score > GPT scores)
t_stat_gpt_3_5, p_value_gpt_3_5 = stats.ttest_rel(sbi_rag_scores, gpt_3_5_scores, alternative='greater')
t_stat_gpt_4, p_value_gpt_4 = stats.ttest_rel(sbi_rag_scores, gpt_4_scores, alternative='greater')

# Compare both p-values with alpha and determine the result for each
result_gpt_3_5 = "Reject the null hypothesis. The mean SBI-RAG Reasoning Score is significantly greater than the mean GPT 3.5 Turbo score." if p_value_gpt_3_5 < alpha else "Fail to reject the null hypothesis."
result_gpt_4 = "Reject the null hypothesis. The mean SBI-RAG Reasoning Score is significantly greater than the mean GPT 4.0 score." if p_value_gpt_4 < alpha else "Fail to reject the null hypothesis."

# Display the results
print(f"SBI-RAG vs GPT 3.5 Turbo: {result_gpt_3_5} (p-value: {p_value_gpt_3_5})")
print(f"SBI-RAG vs GPT 4.0: {result_gpt_4} (p-value: {p_value_gpt_4})")

import pandas as pd
from scipy import stats

# Load the data from the Excel file
file_path = '/content/Paired - t test one tail.xlsx'
excel_data = pd.ExcelFile(file_path)

# Parse the relevant sheet
df = excel_data.parse('Sheet1')

# Extract the columns for the paired t-test
sbi_rag_scores = df['SBI-RAG Reasoning Score '].astype(float)
gpt_4_scores = df['GPT 4.0 Reasoning Score'].astype(float)
gpt_3_5_scores = df['GPT 3.5 Turbo Reasoning Score'].astype(float)

# Perform a one-tailed paired t-test for GPT 4.0 (alternative hypothesis: mean SBI-RAG score > mean GPT 4.0 score)
t_statistic_gpt4, p_value_gpt4 = stats.ttest_rel(sbi_rag_scores, gpt_4_scores, alternative='greater')

# Perform a one-tailed paired t-test for GPT 3.5 Turbo (alternative hypothesis: mean SBI-RAG score > mean GPT 3.5 Turbo score)
t_statistic_gpt3_5, p_value_gpt3_5 = stats.ttest_rel(sbi_rag_scores, gpt_3_5_scores, alternative='greater')

# Perform a one-tailed paired t-test for GPT 4.0 vs GPT 3.5 Turbo
t_statistic_gpt4_new, p_value_gpt4_new = stats.ttest_rel(gpt_4_scores, gpt_3_5_scores, alternative='greater')

# Output the results
alpha = 0.05  # Significance level

print("Comparison of SBI-RAG Reasoning Score with GPT 4.0:")
print(f"t-statistic (GPT 4.0): {t_statistic_gpt4}")
print(f"p-value (GPT 4.0): {p_value_gpt4}")
if p_value_gpt4 < alpha:
    print("Reject the null hypothesis: SBI-RAG scores are significantly higher than GPT 4.0 scores.")
else:
    print("Fail to reject the null hypothesis: No significant difference between SBI-RAG and GPT 4.0 scores.")

print("\nComparison of SBI-RAG Reasoning Score with GPT 3.5 Turbo:")
print(f"t-statistic (GPT 3.5 Turbo): {t_statistic_gpt3_5}")
print(f"p-value (GPT 3.5 Turbo): {p_value_gpt3_5}")
if p_value_gpt3_5 < alpha:
    print("Reject the null hypothesis: SBI-RAG scores are significantly higher than GPT 3.5 Turbo scores.")
else:
    print("Fail to reject the null hypothesis: No significant difference between SBI-RAG and GPT 3.5 Turbo scores.")

print("\nComparison of GPT-4 Reasoning Score with GPT 3.5 Turbo:")
print(f"t-statistic (GPT 3.5 Turbo_vs_GPT_4): {t_statistic_gpt4_new}")
print(f"p-value (GPT 3.5 Turbo_vs_GPT_4): {p_value_gpt4_new}")
if p_value_gpt4_new < alpha:
    print("Reject the null hypothesis: GPT 4.0 scores are significantly higher than GPT 3.5 Turbo scores.")
else:
    print("Fail to reject the null hypothesis: No significant difference between GPT 4.0 and GPT 3.5 Turbo scores.")